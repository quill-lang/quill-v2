\documentclass[UKenglish, 11pt, a4paper, parskip=half]{scrbook}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage{makecell}
\usepackage{microtype}
\usepackage[pdfa]{hyperref}
\hypersetup{colorlinks=true, linktoc=all}
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}

\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{TeX Gyre Pagella}
\setmonofont{Fira Code}[
    Scale=MatchLowercase,
    Contextuals=Alternate  % Activate the calt feature
]
\usepackage{listings}
\usepackage{lstfiracode} % https://ctan.org/pkg/lstfiracode
\lstset{
    style=FiraCodeStyle,   % Use predefined FiraCodeStyle
    basicstyle=\ttfamily   % Use \ttfamily for source code listings
}

\newcommand{\inlinecode}[1]{\lstinline{#1}}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{tikz-cd}
\usepackage[titletoc]{appendix}

\usepackage{mathpartir}
\usepackage{braket}
\usepackage[all,2cell,cmtip]{xy}
\usepackage[capitalize]{cleveref}
\usepackage{aliascnt}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{mathtools}
\include{hott_macros.tex}

\AtBeginEnvironment{appendices}{\crefalias{chapter}{appendix}}

\title{Quill and Feather Language Specifications}
\author{zeramorphic}

\begin{document}

\frontmatter

\maketitle

\tableofcontents

\chapter{Preface}

The Quill language is a large project, and it has proven difficult to manage such a large project without adequate planning and formal specification.
Feature bloat has led the language to a place where its current state is difficult to improve upon incrementally; entire crates of code must be thrown out in order to make meaningful refactors.
This specification aims to codify the functions of each part of code in such a way that when they are implemented, their design is already well thought-out, and further changes can be made without extensive refactoring or modification.

The document begins with defining Quill's aims and requirements, and the consequences this has on the language.
In theory, every feature of Quill should be explained in this document, with a clear rationale for the decisions made and the tradeoffs they imply.
The design process of Quill should be made clear for any reader sufficiently familiar with language design.

It then continues, describing the various language features of Feather and Quill, making formal definitions where useful for further analysis.
A general background knowledge of type theory and lambda calculus, as well with general language design theory, will be assumed, although merits of various type systems will be discussed in depth in future sections.

\chapter{Design principles}

\section{Quill and Feather}

This project aims to codify the languages of Quill and Feather.
They are innately linked, but are not the same.
Put simply, Quill is the user-facing language, which is transpiled into Feather, a functional intermediate language.
This is then compiled to an executable file.
The reasoning behind this choice to separate Quill from Feather is explored later.

Occasionally, we describe features that `Quill has', especially when the separation of responsibility between Quill and Feather is not yet established, however they may apply equally to both languages.
Quill, being the name of the `end product', is simply used as a synonym for the project in its entirety.

\section{Goals}

\subsection{Opinions and facts}

In this section, I will outline several reasons why certain paradigms were chosen for Quill's design.
It is important to emphasise that when I say `favour X over Y', I do not mean that statement as a blanket truth for programming as a discipline; rather, I am designing a language with such features because I wish one were to exist.
The answers to the questions posed below will likely have different answers to different programmers, and different languages likely exist that satisfy those requirements.
Quill is created simply because there is no language that adequately meets the requirements I would wish for a language, it is not a statement that those requirements are objectively good.

\subsection{Outline of goals}

Quill must be, first and foremost, a functional efficient programming language.
It is worth elaborating on the definitions and implications of these words, as well as the reasons for having chosen such words.

\subsection{Functional programming}

By \textit{functional}, we refer to functional programming.
On a macroscopic level, any program accepts some input, performs some calculations, and produces some output.
Where functional and imperative programming languages differ is the ways in which programmers are empowered to perform such calculations.

Functional programs are, in and of themselves, functions.
Those functions are constructed by the composition of other functions, repeatedly applying transformations to input data to convert it into new data, until a final result is calculated, at which point it is returned to the user.
Conversely, an imperative program typically manipulates input data directly, by mutating it, as opposed to transforming it with functions.

There are several advantages to functional programming when compared to imperative programming.
\begin{itemize}
    \item A functional program is, by its very nature, comprised of distinct transformative functions.
    These functions can naturally be easily separated from each other and from the data they operate on.
    The modularity encourages the use of abstraction, and decouples the functions themselves from their implementation details, which allows for potentially easier refactoring.
    \item Combinators abstract away boilerplate.
    Off-by-one errors are difficult to introduce when your loop is encapsulated inside a \inlinecode{map} or \inlinecode{fold}.
    \item In \textit{pure} functional programming, functions have no side effects.
    In particular, they are \textit{deterministic}; a function's output depends only on the function's inputs, regardless of the execution context.
    This allows for far superior static analysis of programs.
    Any expression may be safely and easily evaluated at compile time due to this determinism, which removes the need for complicated \inlinecode{constexpr} mechanics or code colouring.
    \item Immutability is preferred by default.
    Whole transformations of input variables are more idiomatic than gradually updating a variable's contents.
    This prevents bugs which may be introduced by concurrent modification, such as altering the contents of a list while iterating through it.
    Code often operates on the assumption that certain variables will not change; this paradigm codifies and enforces that assumption.
\end{itemize}

\subsection{Programmer versus compiler}

Much of the `benefits' of functional programming outlined above are predicated on a tacit assumption: that it is worth foregoing direct control over the instructions being executed in exchange for fewer bugs and a more robust programming experience.

Fundamentally, every language must define a split of power between the programmer and the compiler.
Languages in the C family often give the programmer direct control over the system's memory, leaving allocation and deallocation up to the programmer.
By allowing the programmer such direct access to the sequence of instructions that will be executed, this forces the programmer to assume the responsibility of writing correct code.
The compiler, to such a programmer, is a tool to take ideas and formalise them into a logical system.

Higher-level languages take a different stance; any gain in performance achieved as a result of direct memory management (for example) is eclipsed by the cognitive load such a system places on the programmer, and the time cost that is incurred when inevitable memory-related bugs are found and must be fixed.

More modern programming languages understand the compiler not as a tool to formalise some already-perfect idea into a logical language, but instead as a partner, which will validate your decisions and ensure that certain logical invariants are upheld.
Of course, this comes at the cost of flexibility---for example, by denying mutability outright, certain algorithms like quicksort become nontrivial to express.

One particularly prominent example is the Rust compiler, notorious for novice users `fighting the borrow checker'.
Rust's borrow checker statically analyses memory use using an ownership-based memory model, and thus eliminates whole classes of bugs at compile-time, such as dangling and null pointers.
This comes at the cost of forcing the programmer to prove to the compiler that memory management is safe at compile time.
The borrow checker is quite conservative in what it accepts, and if in doubt will reject an otherwise valid program.
Certain constructions, which would be idiomatic in related languages like C++, are invalid under the rules of the borrow checker in Rust, and many users struggle to understand why the borrow checker is rejecting clearly working code.
However, once one is familiar with the borrow checker, it becomes clear that it is an ally, not an adversary: delegating some amount of power to the borrow checker is a compromise that allows you to be certain about the memory properties of a program at the expense of absolute freedom.

\subsection{When to compromise with the compiler}

At the heart of Quill is the supposition that it is worth trading absolute freedom for compile-time guarantees, provided that the expense incurred is not `too significant'.
Unfortunately, what tradeoffs are too significant is a problem of opinion, not of logic, so this remains an unsatisfying conclusion to this analysis.
We provide a number of heuristics to guide our future discussion.
Note that these heuristics are very opinionated, and do not apply equally to programming as a whole.
A number of these assertions originate from the `Zen of Zig', a description of the goals of Zig and its community, accessible on the command line by running \inlinecode{zig zen}.

We assert that the language should be built to deal with large projects better than small ones.
Many programmers have experienced the bloat that arises when tools are used for longer than originally intended, and more and more time is spent forcing yet more features into such programs.
It is often the case that a small program unintentionally becomes large; it is rarely the case that a large program unintentionally becomes small.
For this reason, programs should be built with the intent to scale, even if this is not an initial design goal.

Catching bugs at compile time is superior to catching them at runtime.
Clearly, this belief is not held by all programmers; for instance Python does not have a strict notion of compile time and runtime, and almost all errors (except, of course, syntax errors) can occur at any point during program execution.
There is also the disadvantage that an incomplete program cannot be run at all.
There is merit in being able to execute partially completed, logically unsound, programs---provided that this be for the purpose of debugging and aiding development, and not for use in production code.
Languages that have more compile-time guarantees are often harder to get a running program, but often easier to get a correct program once it does run.
Given that a common objective is to create correct and sound code, even if that means more initial time in development, means that we err on the side of compile-time guarantees when possible.

Clarity in reading and understanding code is favoured over ease of writing code.
Code is written only once, but read many times.
By making relations clear between different parts of a program, for example by establishing strict visibility rules, large programs become easier to understand.

Code should be designed in a modular manner.
In any large project, refactoring and redesigning certain modules is inevitable.
The programming language should actively work to make such tasks easier to perform.
By promoting a culture of modular code design, these inevitable processes become significantly less of a burden.
Independent, modular parts are much more simple to reason logically about, when compared to large interlinked processes.
Given the emphasis on reading (and understanding) code over writing it, a modular design is instrumental.

Data should be decoupled from the operations performed on them.
This opposes the paradigm of object-oriented programming, which often ties together any data with its functions.
With languages like Java, this paradigm falls into the problem that all functions \textit{must} be contained with a class, degrading the meaning of a class as an object that may be instantiated with an inheritance hierarchy.
Inheritance-based languages also suffer from the diamond problem.
These issues are not sufficient to abandon the object-oriented paradigm completely---indeed, object-oriented programming has many benefits, especially in particular domains with concrete inheritance hierarchies---but are enough for our purposes to favour an alternate paradigm.

Processes should be designed in a sufficiently abstract way as to know only what it must.
If a function's argument may be of an arbitrary type, there is no use in specifying it; indeed, this can only introduce bugs, when implementation details are unintentionally relied on.
Whenever possible, features like Rust's traits or Haskell's typeclasses should be used to intentionally `forget' unneeded information.

\subsection{Efficient programming}

The word \textit{efficient} carries a double meaning: computational efficiency, and efficiency of writing and reading code.
Quill attempts to satisfy both of these definitions, to an extent.
The commitment to efficiency of reading code follows from the previous asserted goals, but efficiency of writing code does not.
Of course, Quill would ideally be an efficient language to write code in, but this must come at a lower priority.

Under the aforementioned constraints, Quill should be the most \textit{computationally} efficient language possible.
It is important to emphasise here that speed will \textit{never} come at the expense of one of Quill's other goals.
Quill will never become a `functional C'; the level of independence offered over memory management offered (and required) by C conflicts with, for example, the axiom that catching bugs at compile time is better than catching them at runtime.
When such languages as Rust exist, which demonstrate the ability for a modern language to have neither a garbage collector nor unsafe memory access, it is difficult to justify the inclusion of such a feature into Quill.

However, when possible, efficient constructs will be favoured over inefficient ones.
Perhaps most importantly, Quill is a compiled language, not a just-in-time compiled or interpreted language.
This provides a clear speed benefit, and the only cost for the programmer is perhaps an increase in compilation time.

This analysis would imply that the best choice for Quill should be a Rust-like ownership system.
Such a system has not seen prominence in functional programming languages to date, but we intend to show that this is not as a result of incompatibility of the two paradigms---rather, ownership-based memory management is simply a sufficiently new concept such that many languages have not caught up with the development yet.

Denying the ability for the programmer to write computationally efficient imperative constructs places a burden on the compiler to translate idiomatic functional programs into efficient imperative ones.
As a basic example, tail-recursion can be easily translated into iteration.
Constructs such as lambda abstractions and higher-order functions can in theory be translated into ordinary functions whose efficiency matches that of C.

Not all efficiency constraints are in direct opposition to some of Quill's goals.
For example, compile-time optimisation and static analysis is greatly aided by Quill's emphasis on static typing and modular design.

Concurrency may also be implemented in a way that easily aligns with Quill's objectives, by utilising concepts such as the \inlinecode{Send} and \inlinecode{Sync} marker traits from Rust.
Again, compile-time guarantees are used to reduce or eliminate the possibility of runtime bugs.

\section{Requirements}

The above goals naturally lead to the following requirements for Quill as a language.

\subsection{Reliability of performance}

For Quill to be as efficient as possible we require a departure from conventional functional programming models, abandoning thunk-based computation in favour of translation to imperative instructions.
Values will be evaluated eagerly, unless their values are placed behind (for example) a function which must be invoked to compute the value.

In line with adopting an ownership-based memory model, a garbage collector (or a runtime in general) will likely be unnecessary.

\subsection{Immutability by default}

Functions should be pure.
Pure functions lend themselves better to static analysis and static optimisation, eventually leading to more performant programs with fewer bugs.
This naturally entails referential transparency and variable immutability as a key principle.
In a similar way, recursion is to be preferred over iteration.

\subsection{Static type system}

A static type system prevents many classes of runtime bugs, so it should be used in Quill.
Assuming a static type system is used, the functional programming paradigm requires many features in such a type system, such as higher-order functions and higher-kinded types.

Haskell has several language extensions designed to make the type system more expressive, and this comes at the cost of compiler bloat.
An alternative is to manipulate abstractions as values \cite{ScrapYourTypeClasses}.
This has the advantage of allowing the programmer more flexibility when manipulating abstractions, but requires higher-rank functions to be usable directly inside data structures.
For Quill, which will have heavy compile-time code manipulation, this seems to be a good choice.
Such abstractions-as-values can be optimised away statically into plain function calls.

It is also possible to represent types themselves as values \cite{ZigComptimeTypes}.
For a similar reason, this makes sense to implement in Quill.
This will, however, introduce some subtleties about type equivalence and naming.
These will be addressed later.

Arguably, representing everything as a value distils functional programming into its purest form.
Formal type theory often represents types as values, and deals with this by constructing a hierarchy of universes to contain types of types \textit{ad infinitum} \cite[p.~24]{hottbook}.

Thus, we arrive at another axiom for Quill (and hence Feather) to follow: everything is a value.

\subsection{Everything is a value}

In letting types and abstractions be values, the compiler is obligated to perform certain optimisations.
Types cannot be represented in compiled machine code in the same way they can be represented in Feather, so it falls on the compiler to deduce all types at compile time.
Higher-rank functions are used often in abstractions, such as in functors, which have a \inlinecode{map} function generic over any input type.
Clearly, the types used inside the functor must be determined statically and converted to ordinary functions so that types do not need to be represented at runtime.
This also adds the compile-time benefit of optimisation for specific input types; for example, a \inlinecode{memcpy} would be unnecessary if the type being copied is zero-sized.

Multiple dispatch from Julia and related languages can be emulated at compile-time by simply allowing the implementation of an abstraction used to be chosen at the time of the function call, rather than forced by the type system \cite{MultipleDispatch}.
In particular, by providing various implementations of an abstraction to the compiler itself, the compiler can choose the most appropriate abstraction for the task at compile-time using specialisation rules---although since abstractions are values, the programmer is of course free to specify which abstraction to use manually.

\subsection{Code generation}

Since everything is a value, naturally Quill and Feather code should be values.
It stands to reason that syntax extensions themselves should be writable in the same way as normal Quill code, in a functional style.
The compiler should be able to evaluate such functions at compile time.
This foregoes the typical need for a macro system like in C, by simply allowing Quill to be its own macro system.
Rust's procedural macros fill a similar role, in that they use Rust code to generate more Rust code, and can even parse code blocks and apply transformations \cite{ProcMacros}.
In accordance with the principle of modularity, Quill's language design should be minimal yet expressive, and syntax extensions should then provide the necessary conveniences for developing idiomatic functional code.
For example, monadic \inlinecode{do} blocks may not be part of the base language.

% we can't hope to cover all of logic in a spec, just defer to HoTT book
\iffalse{}
\section{Logical systems}

\subsection{Type theory}

To aid in our descriptions and analysis, type theory will be used extensively.
Before blindly applying type theory to Quill, however, we must choose which theory to follow.
The theory that aligns closest to Quill's goals appears to be homotopy type theory.
It represents types and proofs as values, which can be manipulated on the same level as more conventional values.
It also aims to solve a number of issues present in other intuitionistic type theories.

Even though (homotopy) type theory is a powerful tool, some set theory will also be used where it is less verbose than equivalent type-theoretic constructions.
Our use of type theory is practical; it will often make sense to forego formality in pursuit of clarity, provided that the key logical arguments are not lost.

We begin here by presenting an informal description of many of the key principles of homotopy type theory, which can be found in the book `Homotopy Type Theory' \cite{hottbook}.

\begin{defn}
    A \textit{universe} is a collection whose elements are called \textit{types}.
    If \( A \) is an element of universe \( \mathcal U \), we write \( A : \mathcal U \).
\end{defn}
We assert the existence of a hierarchy of universes.
\[ \mathcal U_0 : \mathcal U_1 : \mathcal U_2 : \cdots \]
We can also write \( A : \mathcal U_j \) if \( j \geq i \).
\begin{defn}
    Given a universe \( \mathcal U_i \), a type \( A \) is called \textit{small} if \( A : \mathcal U_i \).
    Assuming indices \( i \) may be assigned in a consistent way in a given context, the indices may be omitted, and we can write \( A : \mathcal U \).
\end{defn}
\begin{defn}
    A \textit{judgment} is a statement of the form \( a : A \).
    If such a judgment is determined to be true, then we say \( a \) has type \( A \).
    A type \( A \) is \textit{inhabited} if there exists an object \( a \) such that \( a : A \).
    A type is uninhabited if there does not exist such an object.
    If \( A \) is a type that represents a proposition under the Curry-Howard correspondence, then if \( a : A \), \( a \) is a \textit{witness} to the provability of \( A \).
\end{defn}

A fundamental notion of type systems is that of equality: given two values \( a \) and \( b \), when can we say that \( a = b \)?
Many theories tackle this notion in different ways.
There are two key notions of equality in type theory, that of judgmental equality and propositional equality \cite[p.~18--19]{hottbook}.
\begin{defn}
    Let \( A \) be a type, and \( a, b : A \).
    Then, the type \( a =_A b \) exists, and is called the \textit{propositional equality type of \( a \) and \( b \)}.
    If this type is inhabited, we say that \( a \) and \( b \) are \textit{propositionally equal}.
\end{defn}
\begin{defn}
    Let \( A \) be a type, and \( a, b : A \).
    If \( a \) and \( b \) are equal by definition, then \( a \) and \( b \) are \textit{equal by definition}, \textit{definitionally equal}, or \textit{judgmentally equal}.
    This is written \( a \equiv b : A \).
    Checking definitional equality is algorithmically computable; it amounts to expanding out the definitions
\end{defn}
By convention \cite[p.~19]{hottbook}, the symbols \( : \) and \( \equiv \) have lowest operator precedence.
However, when using the sequent calculus, we define that the \( , \) and \( \vdash \) symbols, along with wide spacing, have even lower precenence, in that order (highest to lowest).
For instance, the expression \( \Gamma, x : \tau \vdash e : \tau' \) should be interpreted as \( (\Gamma, (x : \tau)) \vdash (e : \tau') \).
\begin{eg}
    The objects \( 2 + 2 \) and \( 4 \) are propositionally equal, since we can construct a proof that \( 2 + 2 = 4 \).
    Conversely, if we define \( a :\equiv 2 : \mathbb N \), then \( a \) and \( 2 \) are equal by definition.
\end{eg}
\begin{defn}
    If \( A \) and \( B \) are types, then \( A \to B \) is the type of functions with domain \( A \) and codomain \( B \).
\end{defn}

\subsection{Sequent calculus}

To formally define typing rules, we use the \textit{sequent calculus}, a form of logical reasoning.
Consider the statement \( \Gamma \vdash \Sigma \), where \( \Gamma \) and \( \Sigma \) are sequences of logical propositions.
This formula states that given the truth of all statements in \( \Gamma \), there exists a proposition in \( \Sigma \) which is true.
\begin{defn}
    An \textit{inference rule} is a formula of the form
    \[ \frac{\Gamma_1 \vdash \Sigma_1\quad \cdots \quad\Gamma_n \vdash \Sigma_n}{\Gamma \vdash \Sigma} \]
    The numerator contains a set of \textit{premises}, all of which must be fulfilled in order for the rule to be applied, after which the \textit{conclusion} on the denominator will be reached.
\end{defn}
\fi{}

\section{High-level design}

In this section, we outline the different parts of the Quill project, their responsibilities, and how they interact.
Each part should be represented as one or more distinct and non-hierarchical modules.
In particular, any shared data between two parts should be realised as a distinct shared module, not by including one part as a dependency of another.
These parts will be presented roughly chronologically in the order they are used to compile a Quill program.

\subsection{Lexer (\inlinecode{lex})}

Quill programs are read from an input file, or standard input.
The resulting stream of Unicode code points is converted into a stream of \textit{tokens}, logically indivisible chunks that have a type.
For example, a symbol such as \inlinecode{/} is a token, and a string such as \inlinecode{"Hello, world!"} is also a token.

Then, token streams are converted into \textit{token trees}, which are pairs of brackets of matching type and their contents, such as \inlinecode{(1 + 2)}.
Naturally, token trees may be nested.

The lexer supports syntax extensions, which alter the rules it uses to classify tokens.
For instance, a syntax extension could be used to convert \inlinecode{10m} into \inlinecode{metres 10}.
The resulting code \inlinecode{metres 10} will be automatically enclosed in a token tree, so that operator precedence is automatically handled, unlike in the C preprocessor.

\subsection{Parser (\inlinecode{parse})}

Token trees from the lexer are analysed to check that they match certain patterns defined either by the Quill compiler or by syntax extensions.
Each pattern may contain sub-patterns that also need to be parsed recursively.
Names are resolved.
This results in an untyped abstract syntax tree.
The nodes of this abstract syntax tree represent expressions or top-level definitions such as functions.
Syntax extensions cannot create new node types.
Many nodes in the syntax tree will be given a type, and most nodes in the tree will provide some typing information for the type checker, such as equivalence relations between types of certain nodes.

\subsection{Type checker (\inlinecode{typeck})}

The type checker infers types of all variables in a given expression, given the constraints from the parser.
This may need to execute arbitrary Quill code, especially when types are manipulated as values.
Thus, this and almost all future parts of the compiler may be executed in parallel.
This results in a typed syntax tree.

\subsection{Feather transpiler (\inlinecode{feathergen})}

Typed syntax trees are converted into Feather, an intermediate language that can be executed and compiled to machine code.
Expressions in Feather are based on a modified version of \textit{administrative normal form} (ANF), taking influence from \textit{K-normal form} (KNF).
This representation allows many code transformations and optimisations to be performed \cite{ANFContinued}, while retaining the semantic content of regions to be used for lifetime analysis \cite{KNF}.

\subsection{Interpreter (\inlinecode{interpret})}

Feather code can be evaluated directly without compilation.
The resulting code will likely run slower than compiled code, but no expensive code generation or linking step is required.
This is used to execute expressions at compile time, such as type manipulation.

\subsection{Monomorphisation (\inlinecode{mono})}

Static analysis on Feather expressions is performed, and all values of runtime-incompatible types, such as types and higher-rank functions, are deduced.
The resulting code is duplicated for each possible (semantically different) value of these values, and usages are updated so that the code no longer depends on higher-rank functions or types as values.
At this stage, implementations of abstractions can be resolved automatically.

\subsection{Code generation (\inlinecode{codegen})}

Feather programs must be compiled into machine code.
This task is handled by the \inlinecode{codegen} module.
At a high level, this module takes monomorphised Feather code and converts it into LLVM IR, which will be passed off to an LLVM compiler which will convert it to machine code.
This process involves the conversion of function paradigms such as recursion into imperative constructs such as iteration, or functional transformations into mutation where applicable.
It also must provide native implementations of standard library functions that cannot be expressed without some interaction with the OS or kernel.
This also acts as a frontend for linking the final object files together into an executable.

\subsection{Diagnostics and error handling (\inlinecode{diag})}

Using this diagnostic infrastructure, errors and warnings may be emitted with reference to the actual code that caused the error, displaying an easy-to-read representation of the error on the command line.

\subsection{Query-based compilation}

To increase and enforce modularity, the functionality each of the above modules is typically represented using queries.
This has seen use in Rust \cite{RustDevGuideOverview}, which has been seen to enhance incremental compilation among other modularity benefits.
This will also empower the compiler to act in parallel.

\mainmatter

\part{Feather Language Specification}

\chapter{Syntax}

\section{Representations}

Feather is a strongly typed, functional, K-normal form intermediate language. It has two representations, an \textit{in-memory} representation, and a \textit{textual} representation.
These two representations fundamentally encode the same data; they are in isomorphism.
Hence, we will discuss here the textual representation, and leave the in-memory representation as an implementation detail.

\section{S-expressions}

\subsection{Atoms}
\label{sec:prim_types}

We begin our discussion of Feather by defining its core building blocks.
We assume the definition of the types \( \mathbb N \) of natural numbers \( 0, 1, \dots \), along with the usual arithmetic operations.
\begin{defn}
    An \textit{\( n \)-bit unsigned integer} is an element of \( \mathbb N \) strictly less than the value \( 2^n \).
    That is, \( x \) is an \( n \)-bit unsigned integer if there exists a value \( y : \mathbb N \) such that \( x + y = 2^n \) and \( y \neq 0 \).
    We construct the type of \( n \)-bit unsigned integers, written \( \mathbb N_n : \mathcal U_0 \).
    To preserve uniqueness of types, we will write \( x_n \) to denote the instance of \( x \) of type \( \mathbb N_n \).
    For clarity, the notation \( x_{\mathbb N_n} \) is also used.
\end{defn}
Let \( \mathbb Z \) be the type of all integers, created by generating a group from \( \mathbb N \) under addition.
\begin{defn}
    An \textit{\( n \)-bit signed integer} is an element of \( \mathbb Z \) not less than \( -2^{n-1} \), and strictly less than \( 2^{n-1} \).
    Likewise we construct the type of \( n \)-bit signed integers, denoted \( \mathbb Z_n : \mathcal U_0 \).
    Again, the notation \( x_n = x_{\mathbb Z_n} \) is used to refer to the instance of \( x \) that has type \( \mathbb Z_n \).
\end{defn}
\begin{defn}
    For \( n = 16, 32, 64, 128, 256 \), we define the \textit{\( n \)-bit floating point} type \( \mathbb F_n : \mathcal U_0 \) to be the quotient of \( \mathbb N_n \) by the equivalence relation defined by the IEEE 754 standard for floating point representation.
    Arithmetic is defined on this type as per the specification \cite{IEEE754}.
\end{defn}
\begin{defn}
    We define the Boolean type \( \mathbb B : \mathcal U_0 \) with elements \( \qty{\top, \bot} \), conventionally named `true' and `false'.
\end{defn}
\begin{defn}
    Let \( \mathbb U : \mathcal U_0 \) be the type of Unicode code points; its elements are 64-bit unsigned integers as defined above, tagged \( x_{\mathbb U} \) such that their type is not simply \( \mathbb N \).
\end{defn}
\begin{defn}
    Let \( \emptyt \) be the \textit{empty type} with no elements.
    Let \( \ttt : \mathcal U_0 \) be the \textit{unit type} with one element \( \ttt : \ttt \).
\end{defn}
These are exactly the primitive types used in Feather.
\begin{defn}
    An \textit{atom} is an element of type \( \mathbb N_n, \mathbb Z_n, \mathbb F_n, \mathbb B, \mathbb U, \emptyt, \ttt \) for any valid \( n \).
    Note that all atoms have small type; that is, their types are all in the space \( \mathcal U_0 \).
\end{defn}

\subsection{Binary representations}
\begin{defn}
    A \textit{(binary) representation} of a type \( A : \mathcal U \) is an injective function
    \[ \rho : A \to \mathbb B^n \]
    for some \( n : \mathbb N \).
    We say that \( \rho \) represents \( A \) in \( n \) bits.
    Since \( \rho \) is injective, this implicitly defines the surjective function \( \rho^{-1} : \mathbb B^n \to A \) such that \( \rho^{-1} \circ \rho \equiv \mathrm{id}_A \).
    Note that if some \( x : \mathbb B^n \) is not contained in the image of \( \rho \), its value under \( \rho^{-1} \) is undefined \textit{a priori}.
\end{defn}
\begin{defn}
    A representation \( \rho : A \to \mathbb B^n \) in \( n \) bits is \textit{minimal} if, for all representations \( \sigma : A \to \mathbb B^m \) in \( m \) bits, we have \( m \geq n \).
    That is, the amount of bits required for the representation of \( \rho \) is minimal.
\end{defn}
\begin{defn}
    The canonical binary expansion for \( \mathbb N_n \), written \( \rho_{\mathbb N_n} \), is the base-2 representation of a number using \( n \) bits, where \( \top \) corresponds to a one, and \( \bot \) corresponds to a zero.
    The representation \( \rho_{\mathbb Z_n} \) works similarly, using two's complement.
    Floating-point representations are constructed in a similar way.
    Booleans are represented in a single bit using the identity map.
    The representation of Unicode code points uses a full 32 bits for convenience of word size.
    The unit type is represented in zero bits.
\end{defn}
It is then trivial to prove the following lemma.
\begin{lem}
    The canonical representations of atomic types, with the exception of \( \mathbb U \), are minimal.
\end{lem}

\subsection{Expressions and schemas}
\begin{defn}
    A \textit{symbol} is a string of Unicode code points, written as a string.
    The type of symbols is \( \mathbb S \).
    When written textually, symbols must be \textit{escaped} in such a way that they are unambiguously symbols.
    If the symbol contains any non-letter characters, symbols should be surrounded with quote marks \inlinecode{"} to disambiguate them.
    Standard C character escaping should be used to represent control characters and quote marks.
\end{defn}
\begin{defn}
    An \textit{S-expression} is either an atom, a symbol, or a list of S-expressions, denoted \inlinecode{(x y ... z)} where \inlinecode{x}, \inlinecode{y}, \inlinecode{z} are S-expressions.
    A \textit{schema} is a prescribed form of an S-expression; it ascribes types to S-expressions that fit a certain form.
    The symbols \inlinecode{*} and \inlinecode{+} are used as in Backus-Naur form.
    Note that \inlinecode{T*}, for example, is not an S-expression alone; it is a list of S-expressions which must be parenthesised in order to form an S-expression.
\end{defn}

\section{Expression syntax}

\subsection{Identifiers}

The following schemas specify identifiers and related types.

\begin{tabular}{r l p{6cm}}
    \inlinecode{Location} \( ::= \) & \inlinecode{(}\( \mathbb N_{32} \) \( \mathbb N_{32} \)\inlinecode{)} & line number, then column number, zero indexed \\
    \inlinecode{Range} \( ::= \) & \inlinecode{(Location Location)} & start location (inclusive), then end location (exclusive) \\
    \inlinecode{Name} \( ::= \) & \( \mathbb S \)\inlinecode{\ | (}\( \mathbb S \)\inlinecode{\ Range)} & the symbol itself then optionally the range at which it was written \\
    \inlinecode{QualifiedName} \( ::= \) & \inlinecode{(}\( \mathbb S \) \inlinecode{+)} & list of one or more name segments
\end{tabular}

A \textit{location} represents a place in some input code.
Since Feather is an IL, locations refer to a place inside a file of Quill code.
This allows error messages and debug symbols to refer to the original code.
A \textit{range} refers to a region of code, typically a single token or token tree.

Feather distinguishes between \textit{names}, which are single symbols representing in-scope items, and \textit{qualified names}, which represent a module hierarchy and then optionally refer to an item inside that hierarchy, depending on context.
Importantly, a qualified name with only one name segment is \textit{not} a name.

\subsection{Primitives}

\begin{tabular}{r l p{7cm}}
    \inlinecode{Primitive} \( ::= \) & \( \mathbb Z_{n} \)\inlinecode{\ |\ }\( \mathbb N_{n} \)\inlinecode{\ |\ }\( \mathbb F_{n} \)\inlinecode{\ |\ }\( \mathbb U \)\inlinecode{\ |\ }\( \mathbb B \)\inlinecode{\ |\ }\( \ttt \) \\
    \inlinecode{PrimitiveType} \( ::= \) & \inlinecode{Int}\( n \)\inlinecode{\ | Uint}\( n \)\inlinecode{\ | Float}\( n \)\inlinecode{\ | Char | Bool | Unit | Empty}
\end{tabular}

The \( n \) above may be substituted for valid integers.

\subsection{Expressions}

The type of expressions is \inlinecode{Expr}.
Expressions are organised into tree structures.
There exists a partial function \( p : \) \inlinecode{Expr} \( \to \) \inlinecode{Expr} which yields the parent expression if it exists.

\begin{defn}
    An expression is called \( n \)-\textit{binding}, or an \( n \)-\textit{binder}, if it creates \( n \) new local variables, assigning them values.
    An expression that binds no new local variables is called \textit{non-binding}.
\end{defn}

\begin{defn}
    The \textit{scope} of an expression is the set of expressions from which it can use variables.
    If there is a parent expression, the scope is the union of the scope of the parent expression together with the set containing the parent expression.
    Otherwise, the scope is the empty set.
    In the context of an expression \( e \), we say that an expression \( f \) is \textit{in scope} if \( f \in \mathrm{scope}(e) \).
\end{defn}

\begin{defn}
    Let \( e \) be an expression, \( b \) be an \( n \)-binding expression in scope of \( e \), and \( m \in \qty{0, \dots, n-1} \).
    The \textit{de Bruijn index} of \( c \) from \( e \), written \( i_e(b, m) \), is a constant referring to the \( m \)th bound variable in the binding expression \( x \).
    In particular,
    \[ i_e(b, m) = \begin{cases}
        i_{p(e)}(b, m) & \text{if } p(e) \neq b \text{ and is non-binding} \\
        i_{p(e)}(b, m) + k & \text{if } p(e) \neq b \text{ exists and is } k \text{-binding} \\
        n - m - 1 & \text{if } p(e) = b \\
    \end{cases} \]
    This definition orders all variables defined in scope in reverse order, assigning them de Bruijn indices of sequential natural numbers.
    Note that \( p(e) \) will always exist and this algorithm will always terminate since \( b \) was defined to be in scope of \( e \).
\end{defn}

Now that the requisite definitions have been made, we can define the schemas for expressions.

\begin{tabular}{r l p{7cm}}
    \inlinecode{DeBruijnIndex} \( ::= \) & \( \mathbb N_{32} \) \\
    \inlinecode{ExprContents} \( ::= \) & \makecell[l]{
        \inlinecode{(local DeBruijnIndex)} \\
        \inlinecode{| (borrow DeBruijnIndex)} \\
        \inlinecode{| (copy Expr)} \\
        \inlinecode{| (inst QualifiedName)} \\
        \inlinecode{| (ap Expr DeBruijnIndex)} \\ % note the A-normal form here
        \inlinecode{| (lambda Expr)} \\
        \inlinecode{| (new QualifiedName Expr*)} \\
        \inlinecode{| (newv QualifiedName\ }\( \mathbb S \)\inlinecode{\ Expr*)} \\ % construct a variant of an enum, the S is the variant name
        \inlinecode{| (destructure\ }\( \mathbb N_{32} \)\inlinecode{\ Expr)} \\ % the number is how many fields to destructure, mostly used for optimising all the de bruijn calculations
        \inlinecode{| (prim Primitive)} \\
        \inlinecode{| (primty PrimitiveType)} \\
        \inlinecode{| (universe Expr)} % The syntax \inlinecode{(universe\ }\( n \)\inlinecode{)} refers to the space \( \mathcal U_n \).
    } \\
    \inlinecode{ExprInfo} \( ::= \) & \inlinecode{(at Range) | (ty Expr) | (named Name)} \\
    \inlinecode{Expr} \( ::= \) & \inlinecode{ExprContents | (ExprInfo* ExprContents)}
\end{tabular}

% TODO: add lifetimes to borrow instructions?
% TODO: add a "create type" expression

Any variant of the \inlinecode{ExprInfo} tag may only be supplied once per expression.

The expressions may have additional information attached to them, denoting the location in code where the expression was written, the variable's type (which itself is an expression), or its name if one was given in Quill code.
The semantic meaning of each expression will be explored later; for now, it suffices to define which expressions are binders.
In particular, \inlinecode{lambda} expressions are 1-binding, \inlinecode{destructure n} expressions are \( n \)-binding, and all other expressions defined here are non-binding.

The absence of \inlinecode{let} expressions does not change the capabilities of the language.
Indeed, in pseudocode, we can apply the following rewrite rule.
\begin{lstlisting}
    let x = e1 in e2 => (lambda x . e2) e1
\end{lstlisting}

\section{Top-level hierarchy}

\subsection{Modules}

Expressions are stored for use in definitions.
Definitions are contained within modules.
Modules are each stored as their own S-expressions, and are not contained within any parent object.

\begin{tabular}{r l p{7cm}}
    \inlinecode{Visibility} \( ::= \) & \inlinecode{pub | priv} \\
    \inlinecode{DefInfo} \( ::= \) & \inlinecode{(at Range) | (ty Expr) | (vis Visibility) | (doc\ }\( \mathbb S \)\inlinecode{)} \\
    \inlinecode{defn} \( ::= \) & \inlinecode{(def DefInfo* Name Expr)} \\
    \inlinecode{ModuleInfo} \( ::= \) & \inlinecode{(file QualifiedName) | (doc\ }\( \mathbb S \)\inlinecode{)} \\
    \inlinecode{Module} \( ::= \) & \inlinecode{(module ModuleInfo* Def*)}
\end{tabular}

If present, a \inlinecode{doc} tag represents documentation for the definition or module in question.
Any variant of the \inlinecode{DefInfo} or \inlinecode{ModuleInfo} tags may only be supplied once per definition or module.
Definitions must have unique names within a module.

\subsection{Projects}

A project is comprised of a hierarchical structure of modules.
Since modules are not stored inside a parent object, the descriptions of modules inside projects simply refer to the name of the module.
The module itself can then be loaded from disk when necessary.
% TODO: sort out dependencies

\begin{tabular}{r l p{7cm}}
    \inlinecode{ProjectInfo} \( ::= \) & \inlinecode{(doc\ }\( \mathbb S \)\inlinecode{)} \\
    \inlinecode{ProjectModule} \( ::= \) & \inlinecode{(module QualifiedName)} \\
    \inlinecode{Project} \( ::= \) & \inlinecode{(project\ }\( \mathbb S \)\inlinecode{\ ProjectInfo* ProjectModule*)} \\
\end{tabular}

Any variant of the \inlinecode{ProjectInfo} tag may only be supplied once.

\section{Examples}
Here is an example of a simple module which exports a function to compute the sum of two integers.
For the purposes of this example, assume the existence of a function \inlinecode{add} of type \( \mathbb Z_{64} \to \mathbb Z_{64} \) stored within the module \inlinecode{core.i64}.
\begin{lstlisting}
(module (doc "Provides a function to compute the sum of two integers.")
    (def (vis pub) (doc "Computes the sum of two integers.") sum
        (lambda (lambda (ap (ap (new core i64 add) 0) 1)))
    )
)
\end{lstlisting}
If this module were saved in the directory \inlinecode{foo} with file name \inlinecode{bar}, a project containing this module might be written
\begin{lstlisting}
(project demo_project
    (doc "An example project.")
    (module (foo bar))
)
\end{lstlisting}

\chapter{Type semantics}

\section{Conventions regarding S-expressions and Feather expressions}

\subsection{Typing}

Previously, we have created types of S-expressions for Feather's syntax.
For example, \inlinecode{(const false)} has type \inlinecode{Expr}.
There is a need to distinguish between the type of an S-expression and the type of the Feather expression it represents.
These two type systems are logically distinct; the existence of an S-expression type does not imply the existence of a corresponding Feather type, or vice versa.

We define \( \mathcal T(e) \) to be the Feather type of an S-expression \( e : \) \inlinecode{Expr}.

Unless otherwise stated, all types in all following sections refer to the Feather type system, so \( x : \tau \) should be interpreted as the judgment `\( x \) is a Feather expression with type \( \tau \)'.
Further, the word `expression' should be taken to mean `Feather expression'.

\subsection{De Bruijn indices}

While de Bruijn indices are useful when processing Feather algorithmically, they are inconvenient to use when discussing the language at a higher level.
To this end, from now on unless otherwise stated, constructs using a de Bruijn index will instead be written as if they use a named variable.
Likewise, binding expressions will be written as if they bind their variables to names.

\section{Type deduction rules}

% TODO: talk about `undefined`

\subsection{The typing environment}

\begin{defn}
    An \textit{assumed judgment} is a pair of the form \( \langle x, \tau \rangle \), where \( x \) is an expression and \( \tau \) is a type.
    A set of assumed judgments, denoted \( \Gamma \), is a \textit{typing environment} or \textit{typing context}.
\end{defn}
Consider the following subset of the Hindley-Milner declarative rule system.
\begin{mathpar}
    \inferrule*[right=Var]{\langle x, \sigma \rangle \in \Gamma}{\Gamma \vdash x : \sigma}
    \and
    \inferrule*[right=App]{\Gamma \vdash e_0 : \tau \to \tau' \quad \Gamma \vdash e_1 : \tau}{\Gamma \vdash e_0\ e_1 : \tau'}
    \and
    \inferrule*[right=Abs]{\Gamma, x : \tau \vdash e : \tau'}{\Gamma \vdash \lambda\ x\ .\ e : \tau \to \tau'}
\end{mathpar}
Note that the conclusions of these rules are judgments; their evaluation is governed by the type system itself.
The use of these three rules, or an equivalent formulation thereof, in an arbitrary functional programming language is largely uncontroversial.
The \textsc{Var} rule is essentially an axiom for building basic typing judgments, given assumed judgments in the typing context.
It is the most basic way of constructing a judgment that an object has a specific type.
The \textsc{App} rule describes the functionality of applying an argument to a function.
If the type of the function's argument matches the function's domain, the function can be applied, and the resulting variable has type equal to the function's codomain.
Finally, the \textsc{Abs} rule performs the reverse operation: it creates an abstraction using a lambda.
The newly created function has a type compatible with the \textsc{App} rule, so that the function can later be evaluated with a particular input.

Note that the types in the above definitions are not restricted to simply small types in \( \mathcal U_0 \).
For instance, application of pre-existent type constructors are already supported through the use of the \textsc{App} and \textsc{Abs} rules, including dependent types.

The remaining Hindley-Milner rules are as follows.
\begin{mathpar}
    \inferrule*[right=Inst]{\Gamma \vdash e : \sigma'\quad \sigma' \sqsubseteq \sigma}{\Gamma \vdash e : \sigma}
    \and
    \inferrule*[right=Gen]{\Gamma \vdash e : \sigma\quad \alpha \notin \mathrm{free}(\Gamma)}{\Gamma \vdash e : \forall \alpha\ .\ \sigma}
\end{mathpar}
These rules concern subtyping and creating generic versions of monotypes.
However, in accordance with the principle to treat types as values, the symbol \( \forall \) should be replaced by a type-theoretic construction that allows for the type quantified to be provided as a function argument.
Thus, these two rules will not be used in Feather.

What now remains is to create rules to allow interactions with the other, more interesting types found in various type theories.
Clearly, it is not a practical idea to try to implement all of homotopy type theory inside a single programming language: most of the concepts will never be useful in any real projects.
In any case, code that makes heavy use of complex type theoretic constructs is likely to become unmaintainable quickly.
A balance must be struck to create a minimal, yet versatile, subset of this type theory.

Further, we require the ability for types to self-reference without the computational complexity of implementing \( \mathsf{W} \)-types.
For example, the coproduct construction \( B = A + B \), where \( A \) is a known type, should be perfectly valid in Feather.
Such styles of construction are used regularly in the creation of \inlinecode{cons} lists, for example.
Thus, we will not use the rules of homotopy type theory directly, and we must design and prove features about our own type system.

\subsection{Discussion of coproducts and universes}
Recursion in a type definition is only feasible due to the existence of coproducts, since they permit the creation of a type \( A + B \) where no object of type \( B \) need be constructed.
It is therefore the coproduct that must be defined with great care to ensure soundness.
We might reason that a type such as \( A = A + \emptyt \) should not be a valid type, since there is no way to construct an object of type \( A \) without already having such an object.
Consider the following two rules from the homotopy type theoretic definition of the coproduct.
\begin{mathpar}
    \inferrule*[right=$+$-\rform]
  {\oftp\Gamma{A}{\UU} \\ \oftp\Gamma{B}{\UU}}
  {\oftp\Gamma{A+B}{\UU}}
  \and
  \inferrule*[right=$+$-\rintro${}_1$]
  {\oftp\Gamma{A}{\UU} \\ \oftp\Gamma{B}{\UU} \\\\ \oftp\Gamma{a}{A}}
  {\oftp\Gamma{\inl(a)}{A+B}}
\end{mathpar}
The formation rule \( + \)-\rform{} specifies that we can only deduce that \( A+B \) is a type once we have already deduced \( A \) and \( B \) are types themselves.
Consider the following variants of those rules.
\begin{mathpar}
    \inferrule*
  {\oftp\Gamma{A}{\UU}}
  {\oftp\Gamma{A+B}{\UU}}
  \and
  \inferrule*
  {\oftp\Gamma{A}{\UU} \\ \oftp\Gamma{a}{A}}
  {\oftp\Gamma{\inl(a)}{A+B}}
\end{mathpar}
Using these inference rules, we may deduce the existence of the type \( A+B \) given only that \( A \) is a type.
This circumvents the original problem with recursive definitions, provided that at least one of \( A \) and \( B \) are known.
This immediately raises questions: does it make sense to consider \( A + e \) a type for an arbitrary object \( e \)?

The pragmatic answer is to say \textit{yes}: it is a type, but such types may not appear in the final form of a program.
A type \( A + B \), where the nature of \( A \) is not known, must only exist during type inference, after which they must have been resolved.

There is another problem, however, in that the universe of \( A + B \) should depend both on the universe of \( A \) and that of \( B \).
In our new formulation, it does not.
We now must consider to what extent an infinite hierarchy of universes is required in Feather.
In all of the typing rules in Appendix A of `Homotopy Type Theory', the universe indices in the antecedents are all arbitrary and exactly match the universe indices in the consequents, with the exceptions of the \UU-\textsc{intro} and \UU-\textsc{cumul} rules \cite{hottbook}.
This leads us to consider an alternative type theory which contains exactly one universe \UU{}, such that \( \UU : \UU \).
Due to the lack of \( \mathsf{W} \)-types, this is unlikely on its own to lead to unsoundness.

\subsection{Structural rules}

Although we skip over many of the minutiae of rigorously defining a type system, we must state the following well-known structural rules \cite{hottbook}.
Note that instead of formulating these laws using the logic of bound and unbound variables, we will use a function-based approach.
The semantic difference is irrelevant for our practical purposes.
We begin with the first Hindley-Milner rule, \textsc{Var}.

\begin{mathpar}
\inferrule*[right=Var]{\langle x, \sigma \rangle \in \Gamma}{\Gamma \vdash x : \sigma}
\end{mathpar}
Judgmental equality is an equivalence relation, respected by typing.
\begin{mathparpagebreakable}
\inferrule*{\oftp\Gamma{a}{A}}{\jdeqtp\Gamma{a}{a}{A}}
\and
\inferrule*{\jdeqtp\Gamma{a}{b}{A}}{\jdeqtp\Gamma{b}{a}{A}}
\and
\inferrule*{\jdeqtp\Gamma{a}{b}{A} \\ \jdeqtp\Gamma{b}{c}{A}}{\jdeqtp\Gamma{a}{c}{A}}
\and
\inferrule*{\oftp\Gamma{a}{A} \\ \jdeqtp\Gamma{A}{B}{\UU}}{\oftp\Gamma{a}{B}}
\and
\inferrule*{\jdeqtp\Gamma{a}{b}{A} \\ \jdeqtp\Gamma{A}{B}{\UU}}{\jdeqtp\Gamma{a}{b}{B}}
\end{mathparpagebreakable}

\subsection{Dependent function types (\texorpdfstring{$\Pi$}{Π}-types)}

Where bound variables must be used, the syntax \( x.b \) specifically states that the name \( x \) is bound in expression \( b \).
No variables are bound unless stated explicitly.
The type family \( B:A \to \UU \) is used frequently to circumvent the use of bound variables.
This is somewhat circular; \( B:A \to \UU \) is itself a function type, but this should not matter in practice.

\begin{mathparpagebreakable}
  \def\premise{\oftp{\Gamma}{A}{\UU} \and \oftp{\Gamma}{B}{A \to \UU}}
  \inferrule*[right=$\Pi$-\rform]
    \premise
    {\oftp\Gamma{\tprd{x:A}B(x)}{\UU}}
\and
  \inferrule*[right=$\Pi$-\rintro]
  {\oftp{\Gamma}{A}{\UU} \and \oftp{\Gamma}{B}{A \to \UU} \and \oftp{\Gamma,\tmtp xA}{x.b}{B(x)}}
  {\oftp\Gamma{\lam{x:A} (x.b)}{\tprd{x:A} B(x)}}
\and
  \inferrule*[right=$\Pi$-\relim]
  {\oftp{\Gamma}{B}{A \to \UU} \\ \oftp\Gamma{f}{\tprd{x:A} B(x)} \\ \oftp\Gamma{a}{A}}
  {\oftp\Gamma{f(a)}{B(a)}}
\and
  \inferrule*[right=$\Pi$-\rcomp]
  {\oftp{\Gamma}{B}{A \to \UU} \\ \oftp{\Gamma,\tmtp xA}{x.b}{B(x)} \\ \oftp\Gamma{a}{A}}
  {\jdeqtp\Gamma{(\lam{x:A} (x.b))(a)}{a.b}{B(a)}}
\and
  \inferrule*[right=$\Pi$-\runiq]
  {\oftp\Gamma{f}{\tprd{x:A} B(x)}}
  {\jdeqtp\Gamma{f}{(\lam{y:A}f(y))}{\tprd{x:A} B(x)}}
\end{mathparpagebreakable}

Now that dependent function types have been created, the need for bound variables is no longer present.
Due to the use of functions instead of bound variables, the elimination and computation rules for this type, as well as the unit type and other primitive types, will not be needed.

\subsection{The empty type \texorpdfstring{$\emptyt$}{0}}
Here, the antecedent \( \wfctx \Gamma \) is used to denote simply that \( \Gamma \) is a valid typing context, since no other antecedents are present.
\begin{mathparpagebreakable}
  \inferrule*[right=$\emptyt$-\rform]
  {\wfctx\Gamma}
  {\oftp\Gamma\emptyt{\UU}}
\and
  \inferrule*[right=$\emptyt$-\relim]
  {\oftp{\Gamma}{C}{\emptyt \to \UU} \\ \oftp\Gamma{a}{\emptyt}}
  {\oftp\Gamma{\ind{\emptyt}(C,a)}{C(a)}}
\end{mathparpagebreakable}

\subsection{The unit type \texorpdfstring{$\unit$}{1}}
\begin{mathparpagebreakable}
  \inferrule*[right=$\unit$-\rform]
  {\wfctx\Gamma}
  {\oftp\Gamma\unit{\UU}}
\and
  \inferrule*[right=$\unit$-\rintro]
  {\wfctx\Gamma}
  {\oftp\Gamma{\ttt}{\unit}}
\and
  \inferrule*[right=$\unit$-\relim]
  {\oftp{\Gamma}{C}{\unit \to \UU} \\
   \oftp{\Gamma}{c}{C(\ttt)} \\
   \oftp\Gamma{a}{\unit}}
  {\oftp\Gamma{\ind{\unit}(C,c,a)}{C(a)}}
\and
  \inferrule*[right=$\unit$-\rcomp]
  {\oftp{\Gamma}{C}{\unit \to \UU} \\
   \oftp{\Gamma}{c}{C(\ttt)}}
  {\jdeqtp\Gamma{\ind{\unit}(C,c,\ttt)}{c}{C(\ttt)}}
\end{mathparpagebreakable}

\subsection{Primitive types}
Feather has several primitive types that have not yet been discussed.
In the following section, let \( \mathbb P \) denote a primitive type as defined in \cref{sec:prim_types}.
\begin{mathparpagebreakable}
  \inferrule*[right=$\mathbb P$-\rform]
  {\wfctx\Gamma}
  {\oftp\Gamma{\mathbb P}{\UU}}
\and
  \inferrule*[right=$\mathbb P$-\rintro]
  {\wfctx\Gamma}
  {\oftp\Gamma{p}{\mathbb P}}
\and
  \inferrule*[right=$\mathbb P$-\relim]
  {\oftp{\Gamma}{C}{\mathbb P \to \UU} \\
   \oftp{\Gamma}{c}{\tprd{p:\mathbb P} C(p)} \\
   \oftp\Gamma{a}{\mathbb P}}
  {\oftp\Gamma{\ind{\mathbb P}(C,c,a)}{C(a)}}
\and
  \inferrule*[right=$\mathbb P$-\rcomp]
  {\oftp{\Gamma}{C}{\mathbb P \to \UU} \\
   \oftp{\Gamma}{c}{\tprd{p:\mathbb P} C(p)}}
  {\jdeqtp\Gamma{\ind{\mathbb P}(C,c,p)}{c}{C(p)}}
\end{mathparpagebreakable}

\subsection{Dependent pair types (\texorpdfstring{$\Sigma$}{Σ}-types)}
\begin{mathparpagebreakable}
  \def\premise{\oftp{\Gamma}{A}{\UU} \and \oftp{\Gamma}{B}{A \to \UU}}
  \inferrule*[right=$\Sigma$-\rform]
    \premise
    {\oftp\Gamma{\tsm{x:A} B(x)}{\UU}}
  \and
  \inferrule*[right=$\Sigma$-\rintro]
    {\oftp{\Gamma}{B}{A \to \UU} \\
     \oftp\Gamma{a}{A} \\ \oftp\Gamma{b}{B(a)}}
    {\oftp\Gamma{\tup ab}{\tsm{x:A} B(x)}}
  \and
  \inferrule*[right=$\Sigma$-\relim]
    {\oftp{\Gamma}{B}{A \to \UU} \\
     \oftp{\Gamma}{C}{\tsm{x:A} B(x) \to \UU} \\
     \oftp{\Gamma}{g}{\tprd{x:A} \tprd{y:B} C(\tup x y)} \\
     \oftp\Gamma{p}{\tsm{x:A} B(x)}}
    {\oftp\Gamma{\ind{\tsm{x:A} B(x)}(C,g,p)}{C(p)}}
  \and
  \inferrule*[right=$\Sigma$-\rcomp]
    {\oftp{\Gamma}{B}{A \to \UU} \\
     \oftp{\Gamma}{C}{\tsm{x:A} B(x) \to \UU} \\\\
     \oftp{\Gamma}{g}{\tprd{x:A} \tprd{y:B} C(\tup x y)} \\
     \oftp\Gamma{a}{A} \\ \oftp\Gamma{b}{B(a)}}
    {\jdeqtp\Gamma{\ind{\tsm{x:A} B(x)}(C,g,\tup{a}{b})}{g(a)(b)}{C(\tup a b)}}
\end{mathparpagebreakable}

\subsection{Coproduct types}

\begin{mathparpagebreakable}
  \inferrule*[right=$+$-\rform${}_1$]
  {\oftp\Gamma{A}{\UU}}
  {\oftp\Gamma{A+B}{\UU}}
\and
  \inferrule*[right=$+$-\rform${}_2$]
  {\oftp\Gamma{B}{\UU}}
  {\oftp\Gamma{A+B}{\UU}}
\\
  \inferrule*[right=$+$-\rintro${}_1$]
  {\oftp\Gamma{A}{\UU} \\ \oftp\Gamma{a}{A}}
  {\oftp\Gamma{\inl(a)}{A+B}}
\and
  \inferrule*[right=$+$-\rintro${}_2$]
  {\oftp\Gamma{B}{\UU} \\ \oftp\Gamma{b}{B}}
  {\oftp\Gamma{\inr(b)}{A+B}}
\\
  \inferrule*[right=$+$-\relim]
  {\oftp{\Gamma}{C}{A+B \to \UU} \\\\
   \oftp{\Gamma}{c}{\tprd{x:A} C(\inl(x))} \\
   \oftp{\Gamma}{d}{\tprd{y:B} C(\inr(y))} \\\\
   \oftp\Gamma{e}{A+B}}
  {\oftp\Gamma{\ind{A+B}(C,c,d,e)}{C(e)}}
\and
  \inferrule*[right=$+$-\rcomp${}_1$]
  {\oftp{\Gamma}{C}{A+B \to \UU} \\
   \oftp{\Gamma}{c}{\tprd{x:A} C(\inl(x))} \\
   \oftp{\Gamma}{d}{\tprd{y:B} C(\inr(y))} \\\\
   \oftp\Gamma{a}{A}}
  {\jdeqtp\Gamma{\ind{A+B}(C,c,d,\inl(a))}{c(a)}{C(\inl(a))}}
\and
  \inferrule*[right=$+$-\rcomp${}_2$]
  {\oftp{\Gamma}{C}{A+B \to \UU} \\
   \oftp{\Gamma}{c}{\tprd{x:A} C(\inl(x))} \\
   \oftp{\Gamma}{d}{\tprd{y:B} C(\inr(y))} \\\\
   \oftp\Gamma{b}{B}}
  {\jdeqtp\Gamma{\ind{A+B}(C,c,d,\inr(b))}{d(b)}{C(\inr(b))}}
\end{mathparpagebreakable}

\subsection{Newtypes}

There are certain instances where we wish to consider two types `the same' or `different' depending on semantic context.
Two types that are isomorphic are not necessarily to be considered identical due to semantic context.
This is known as the \inlinecode{newtype} idiom.
We will allow types to be uniquely named by defining the following rules.
Let \( \mathbb S^+ \) be a type of names.

\begin{mathparpagebreakable}
  \inferrule*[right=$\mathcal N$-\rform]
    {\oftp{\Gamma}{A}{\UU} \and \oftp{\Gamma}{N}{\mathbb S^+}}
    {\oftp\Gamma{\mathcal N_N(A)}{\UU}}
  \and
  \inferrule*[right=$\mathcal N$-\rintro]
    {\oftp{\Gamma}{A}{\UU} \\
    \oftp{\Gamma}{N}{\mathbb S^+} \\ \oftp\Gamma{a}{A}}
    {\oftp\Gamma{\nu_N(a)}{\mathcal N_N(A)}}
  \and
  \inferrule*[right=$\mathcal N$-\relim]
    {\oftp{\Gamma}{A}{\UU} \\
     \oftp{\Gamma}{C}{\mathcal N_N(A) \to \UU} \\\\
     \oftp{\Gamma}{g}{\tprd{x:A} C(\nu_n(x))} \\
     \oftp\Gamma{p}{\mathcal N_N(A)}}
    {\oftp\Gamma{\ind{\mathcal N_N(A)}(C,g,p)}{C(p)}}
  \and
  \inferrule*[right=$\mathcal N$-\rcomp]
    {\oftp{\Gamma}{A}{\UU} \\
     \oftp{\Gamma}{C}{\mathcal N_N(A) \to \UU} \\\\
     \oftp{\Gamma}{g}{\tprd{x:A} C(\nu_n(x))} \\
     \oftp\Gamma{a}{A}}
    {\jdeqtp\Gamma{\ind{\mathcal N_N(A)}(C,g,\nu_N(a))}{g(a)}{C(\nu_N(a))}}
\end{mathparpagebreakable}

% TODO: prove uniqueness rules

\iffalse{}

We have included a subset of the logical inference rules provided by `Homotopy Type Theory' \cite{hottbook} in \cref{ch:formalisation_reference}.
The names and properties of such rules will be used throughout the rest of this specification.
All of the above Hindley-Milner typing rules will be replaced by formal type-theoretic rules.
For instance, the Hindley-Milner \textsc{Var} rule is superseded by the \(\Vble\) structural rule, which contains fundamentally the same information yet is written from a type-theoretic perspective instead of a set-theoretic viewpoint.

\subsection{Non-primitive types}

As stated in `Homotopy Type Theory' \cite{hottbook}, each type is governed by formation rules, introduction rules, elimination rules, computation rules, and optionally uniqueness principles.

% TODO: explain things

The laws of \textit{dependent function types} are governed by the rules \( \Pi \)-\rform, \( \Pi \)-\rintro, \( \Pi \)-\relim, \( \Pi \)-\rcomp, and \( \Pi \)-\runiq.
The \( \Pi \)-\rintro{} rule is a dependently typed version of the \textsc{Abs} rule defined above, and \( \Pi \)-\relim{} is likewise a dependently typed version of \textsc{App}.
One of the requirements for \( \Pi \)-\rintro{} is \textit{very} restrictive; for all \( x:A \) we must have \( b:B(x) \), which is a type that depends on the value of \( x \).
Some special machinery will need to be designed in Feather's compiler in order to deal with such an expression.

\textit{Dependent pairs} are similarly governed by \( \Sigma \)-\rform, \( \Sigma \)-\rintro, \( \Sigma \)-\relim, and \( \Sigma \)-\rcomp.
Suppose \( \alpha : C \to A \), \( \beta : C \to B \), and \( \varphi : (C \to A) \to (C \to B) \to (C \to A \times B) \).
Then the following commutative diagram defines the function \( \varphi \).
This also implies that the projection functions for products are the dual to the injective functions for coproducts.

\begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQBBEAX1PU1z5CKAEwVqdJq3YAhHnxAZseAkXKliEhizaJOAHX14AtvAAEc3v2VC1pEVqm6QAYR4SYUAObwioAGYAThDGSGIgOBBIZJI67IbG9DgAFoHGwGiB3AD65CA0jPQARjCMAAoCKsIggVheyTjyAcGhiOGRSOqx0noJSanpmTki+SCFJeWVtnqMMP6NViBBIUgAzDQdiDHaPSCGTGjJ9KPjpRU2qjNzCwrLresRUYhdO86GJTjHBcVnU5c1dQaTSWLTWGye4Ve8X0DEChywhjM+0Yh3oiPeME+Jx+kwu1SwYGwsHc3CAA
    \begin{tikzcd}
        & A\times B \arrow[ld, "\proj 1"'] \arrow[rd, "\proj 2"]                           &   \\
    A &                                                                                              & B \\
        & C \arrow[lu, "\alpha"] \arrow[ru, "\beta"'] \arrow[uu, "\varphi\ \alpha\ \beta" description] &
    \end{tikzcd}
\end{center}

\textit{Coproducts}, or \textit{disjoint unions}, are governed by \( + \)-\rform, \( + \)-\rintro\({}_1\), \( + \)-\rintro\({}_2\), \( + \)-\relim, \( + \)-\rcomp\({}_1\), and \( + \)-\rcomp\({}_2\).
The functions \( \inl \) and \( \inr \) are the left and right \textit{injections}, that allow creation of the coproduct type.
The function \( \rec{A + B} \) is the \textit{recursor}, which allows us to operate on the components of the coproduct.
% TODO: how did we get the recursor - only the inductive function was defined!
The recursor is dual to \( \varphi \) defined above.
Let \( \alpha \colon A \to C \) and \( \beta \colon B \to C \), then the following diagram commutes.

\begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQBBEAX1PU1z5CKAEwVqdJq3YAhHnxAZseAkXKliEhizaJOAajm9+yoWtIitU3SADCPCTCgBzeEVAAzAE4QAtkjIQHAgkMUkddgAdSN96HAALL19gLDBGbnlPH39EdSCQxDDtaT1o2ISklLAvDJpGegAjGEYABQEVYRAvLGd4nEyQbz8AmmCkAGYaYptopjR4+hA6xua201U9bt7+40HspDyxxEnwkpBoppxF3aGcsKOT6aiYuMTkrxgAY25ogAJZxjzeh-C4wK5LED1JqtdpmPSpbCwBzcIA
\begin{tikzcd}
    & A+B \arrow[dd, "\rec{A+B}\ \alpha\ \beta" description] &                                                   \\
A \arrow[ru, "\inl"] \arrow[rd, "\alpha"'] &                                                           & B \arrow[lu, "\inr"'] \arrow[ld, "\beta"] \\
    & C                                                         &
\end{tikzcd}
\end{center}

The empty type is governed by \( \emptyt \)-\rform{} and \( \emptyt \)-\relim.

The unit type is governed by \( \unit \)-\rform, \( \unit \)-\rintro, \( \unit \)-\relim, and \( \unit \)-\rcomp.
\fi{}

\begin{appendices}
    \appendixpage
    \noappendicestocpagenum
    \addappheadtotoc

    \chapter{Type theory formalisation reference}
    \label{ch:formalisation_reference}

    For the purposes of discussion, we present many of the rules listed in `Homotopy Type Theory' \cite{hottbook}.

    \input{hott_rules.tex}
\end{appendices}

\backmatter

\printbibliography

\end{document}
